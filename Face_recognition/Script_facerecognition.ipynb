{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-19T17:33:14.752773Z",
     "start_time": "2024-11-19T17:33:13.779768Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import supervision as sv\n",
    "from PIL import Image"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T17:33:17.597674Z",
     "start_time": "2024-11-19T17:33:16.951690Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Initialize MediaPipe FaceMesh model\n",
    "model = mp.solutions.face_mesh.FaceMesh()\n",
    "\n",
    "# Load and convert the image\n",
    "image_path = '/home/juan-carlos/Dropbox/example.JPG'\n",
    "image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "# Convert image to numpy array\n",
    "image_np = np.array(image)\n",
    "\n",
    "# Process the image to get face mesh results\n",
    "results = model.process(image_np)\n",
    "\n",
    "# Check if results are not empty\n",
    "if results.multi_face_landmarks:\n",
    "    # Initialize EdgeAnnotator\n",
    "    edge_annotator = sv.EdgeAnnotator(color=sv.Color.BLACK, thickness=2)\n",
    "\n",
    "    # Convert MediaPipe results to KeyPoints\n",
    "    key_points = sv.KeyPoints.from_mediapipe(results, resolution_wh=image.size)\n",
    "\n",
    "    # Annotate the image\n",
    "    annotated_image = edge_annotator.annotate(scene=image, key_points=key_points)\n",
    "\n",
    "    # Save or display the annotated image\n",
    "    annotated_image.show()  # Or use annotated_image.save('output_path.jpg')\n",
    "else:\n",
    "    print(\"No face landmarks found in the image.\")"
   ],
   "id": "35fc7f8a161c2797",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1732037597.015226   49634 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1732037597.017733   49693 gl_context.cc:357] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.0.9-0ubuntu0.2), renderer: Mesa Intel(R) UHD Graphics 620 (KBL GT2)\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1732037597.043973   49687 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1732037597.059783   49687 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1732037597.074911   49681 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n",
      "UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "72e0100b26fa77d4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "fe01a6cad7c18467"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
